"""Each .grib file in the HRRR dataset contains dozens or hundreds of distinct variables
that represent data along several dimensions. The inventory files published by NOAA are
useful for the human-readable descriptions, but more reliable inventory dataframes can 
be generated by reading the sidecar .grib2.idx files.

The functions in this module generate the metadata required to define the coordinates
along the forecast_time x level dimensions on which specific variables have data. These
dataframes are used to populate the datacube extension metadata for each collection.

The dimensions of interest are:
1. forecast time: either the average, minimum, maximum, or accumulated value for a 
    specific time range, e.g. 3-4 hours, 0-1 day, etc.
    For forecast hour 0, the level is "analysis"
2. level: the models generate predictions of many of the variables for various levels 
    in the atmosphere, e.g. 0-9000 ft, cloud surface, top of atmosphere, etc.
"""
import logging
import multiprocessing as mp
from datetime import datetime, timedelta
from io import StringIO
from pathlib import Path
from typing import Optional

import httpx
import pandas as pd
from stactools.noaa_hrrr.constants import (
    CLOUD_PROVIDER_CONFIGS,
    DATA_DIR,
    PRODUCT_CONFIGS,
    REGION_CONFIGS,
    CloudProvider,
    ForecastCycleType,
    ForecastHourSet,
    Product,
    Region,
)

INVENTORY_CSV_GZ_FORMAT = "__".join(
    [
        "inventory",
        "{region}",
        "{product}.csv.gz",
    ]
)

FORECAST_HOUR = "forecast_hour"
INVENTORY_COLS = [
    "grib_message",
    "variable",
    "level",
    "forecast_time",
]
DESCRIPTION_COLS = [
    "description",
    "unit",
]

# URLs for the published inventory for each region/product/forecast_hour_set from NOAA
# https://www.nco.ncep.noaa.gov/pmb/products/hrrr/#CO
NOAA_INVENTORY_URLS = {
    (
        Region.conus,
        Product.prs,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf00.grib2.shtml",
    (
        Region.conus,
        Product.prs,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf02.grib2.shtml",
    (
        Region.conus,
        Product.nat,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.grib2.shtml",
    (
        Region.conus,
        Product.nat,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf02.grib2.shtml",
    (
        Region.conus,
        Product.sfc,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.grib2.shtml",
    (
        Region.conus,
        Product.sfc,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.grib2.shtml",
    (
        Region.conus,
        Product.subh,
        ForecastHourSet.FH00,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf00.grib2.shtml",
    (
        Region.conus,
        Product.subh,
        ForecastHourSet.FH01_18,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf02.grib2.shtml",
    (
        Region.alaska,
        Product.prs,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.prs,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfprsf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.nat,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.nat,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.sfc,
        ForecastHourSet.FH00_01,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.sfc,
        ForecastHourSet.FH02_48,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsfcf02.ak.grib2.shtml",
    (
        Region.alaska,
        Product.subh,
        ForecastHourSet.FH00,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf00.ak.grib2.shtml",
    (
        Region.alaska,
        Product.subh,
        ForecastHourSet.FH01_18,
    ): "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfsubhf02.ak.grib2.shtml",
}

# dummy datetime for generating inventory csv files
dummy_datetime = datetime(year=2024, month=5, day=1)


def load_inventory_df(
    region: Region,
    product: Product,
    forecast_hour: Optional[int] = None,
) -> pd.DataFrame:
    """Load variable inventory DataFrame

    Load the inventory of variables with descriptions, units, and level/forecast time
    dimension values for a given region/product/forecast_hour_set/forecast_cycle_type
    """
    inventory_df = pd.read_csv(
        DATA_DIR
        / INVENTORY_CSV_GZ_FORMAT.format(
            region=region.value,
            product=product.value,
        ),
        index_col=FORECAST_HOUR,
    )

    # optionally subset down to a specific forecast hour
    if forecast_hour is not None:
        inventory_df = inventory_df.loc[forecast_hour]

    return inventory_df


# Define custom exceptions
class NotFoundError(Exception):
    pass


def read_idx(
    region: Region,
    product: Product,
    cloud_provider: CloudProvider,
    reference_datetime: datetime,
    forecast_hour: int,
) -> pd.DataFrame:
    """Read the contents of a .idx file, heavily cribbed from Herbie"""
    region_config = REGION_CONFIGS[region]
    cloud_provider_config = CLOUD_PROVIDER_CONFIGS[cloud_provider]

    idx_url = cloud_provider_config.url_base + region_config.format_grib_url(
        product=product,
        reference_datetime=reference_datetime,
        forecast_hour=forecast_hour,
        idx=True,
    )

    read_this_idx = None

    response = httpx.get(idx_url, timeout=20)

    try:
        response.raise_for_status()  # Raise an HTTPStatusError for 4xx/5xx status codes
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 404:
            response.close()
            raise NotFoundError(f"404 Not Found: {idx_url}")
        else:
            response.close()
            raise e

    read_this_idx = StringIO(response.text)
    response.close()

    df = pd.read_csv(
        read_this_idx,
        sep=":",
        names=[
            "grib_message",
            "start_byte",
            "reference_time",
            "variable",
            "level",
            "forecast_time",
            "?",
            "??",
            "???",
        ],
    )

    # Format the DataFrame
    df["reference_time"] = pd.to_datetime(df.reference_time, format="d=%Y%m%d%H")
    df["valid_time"] = df["reference_time"] + timedelta(hours=forecast_hour)
    df["start_byte"] = df["start_byte"].astype(int)
    df["byte_size"] = (df["start_byte"].shift(-1) - df["start_byte"]).astype(
        pd.Int64Dtype()
    )
    df = df.reindex(
        columns=[
            "grib_message",
            "start_byte",
            "byte_size",
            "reference_time",
            "valid_time",
            "variable",
            "level",
            "forecast_time",
            "?",
            "??",
            "???",
        ]
    )

    return df.dropna(how="all", axis=1)


def generate_single_inventory_df(
    region: Region,
    product: Product,
    cycle_run_hour: int,
    forecast_hour: int,
) -> pd.DataFrame:
    """Generate a single inventory DataFrame

    Loads the inventory dataframe from a .idx file"""
    idx_df = read_idx(
        region=region,
        product=product,
        cloud_provider=CloudProvider.azure,
        reference_datetime=dummy_datetime + timedelta(hours=cycle_run_hour),
        forecast_hour=forecast_hour,
    )
    out = idx_df.assign(
        region=region.value,
        product=product.value,
        forecast_hour=forecast_hour,
    )[[FORECAST_HOUR] + INVENTORY_COLS]

    assert isinstance(out, pd.DataFrame)

    return out


def generate_inventory_csv_gzs(dest_dir: Path) -> None:
    """Generate all inventory dataframes and write to .csv.gz files"""
    forecast_cycle_type = ForecastCycleType("extended")
    cycle_run_hour = 0
    for region in Region:
        for product in Product:
            product_config = PRODUCT_CONFIGS[product]
            inventory_dfs = []
            allowed_forecast_hours = list(forecast_cycle_type.generate_forecast_hours())

            for forecast_hour_set in product_config.forecast_hour_sets:
                tasks = []
                for forecast_hour in list(
                    set(forecast_hour_set.generate_forecast_hours())
                    & set(allowed_forecast_hours)
                ):
                    tasks.append(
                        (
                            region,
                            product,
                            cycle_run_hour,
                            forecast_hour,
                        )
                    )

                with mp.Pool() as pool:
                    dfs = pool.starmap(generate_single_inventory_df, tasks)

                inventory_df = pd.concat(dfs)

                n_rows = inventory_df.shape[0]

                # get the variable descriptions from the NOAA inventory tables
                noaa_inventory = pd.read_html(
                    NOAA_INVENTORY_URLS[region, product, forecast_hour_set],
                )[1]

                noaa_inventory[["description", "unit"]] = noaa_inventory[
                    "Description"
                ].str.extract(r"(.+?) \[(.+?)\]")

                variable_descriptions = (
                    noaa_inventory[["Parameter", "description", "unit"]]
                    .drop_duplicates()
                    .rename(columns={"Parameter": "variable"})
                )

                # add the variable descriptions
                inventory_df = inventory_df.merge(
                    variable_descriptions, on="variable", how="left"
                )
                assert inventory_df.shape[0] == n_rows

                inventory_dfs.append(inventory_df)

            inventory_csv_gz = dest_dir / INVENTORY_CSV_GZ_FORMAT.format(
                region=region.value,
                product=product.value,
            )
            pd.concat(inventory_dfs).to_csv(inventory_csv_gz, index=False)

            logging.info(f"Data successfully written to {inventory_csv_gz}")
